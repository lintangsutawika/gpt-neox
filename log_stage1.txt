Setting ds_accelerator to cuda (auto detect)
NeoXArgs.from_ymls() ['/mnt/hdd-0/alyssaloo/gpt-neox/configs/pythia/160M.yml']
-------------------- arguments --------------------
  attention_config ................ ['flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash']updated
  attention_dropout ............... 0...........................updated
  batch_size ...................... 32..........................updated
  bias_gelu_fusion ................ True........................updated
  checkpoint_activations .......... True........................updated
  checkpoint_factor ............... 1000........................updated
  clip_grad ....................... 1.0.........................updated
  config_files .................... {'160M.yml': '{\n  "vocab-file": /mnt/ssd-1/data/20B_tokenizer.json,\n  "data_path": "/mnt/ssd-1/data/pile_20B_tokenizer/pile_20B_tokenizer_text_document",\n  "load": "/mnt/hdd-0/alyssaloo/pythia-v2-160M",\n\n  # parallelism settings\n  "pipe_parallel_size": 1,\n  "model_parallel_size": 1,\n\n  "num_layers": 12,\n  "hidden_size": 768,\n  "num_attention_heads": 12,\n  "seq_length": 2048,\n  "max_position_embeddings": 2048,\n  "pos_emb": "rotary",\n  "rotary_pct": 0.25,\n  "no_weight_tying": true,\n  "gpt_j_residual": true,\n  "output_layer_parallelism": "column",\n\n  "attention_config": [[["flash"], 12]],\n\n  "scaled_upper_triang_masked_softmax_fusion": true,\n  "bias_gelu_fusion": true,\n\n  "init_method": "small_init",\n  "output_layer_init_method": "wang_init",\n\n  "optimizer": {\n    "type": "Adam",\n    "params": {\n      "lr": 0.0006,\n      "betas": [0.9, 0.95],\n      "eps": 1.0e-8\n    }\n  },\n  "min_lr": 0.00006,\n\n  "zero_optimization": {\n    "stage": 1, #changed from 1\n    "allgather_partitions": true,\n    "allgather_bucket_size": 500000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 500000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n  "train_micro_batch_size_per_gpu": 32,\n  "gas": 1,\n  "data_impl": "mmap",\n  "num_workers": 1,\n\n  "checkpoint_activations": true,\n  "checkpoint_num_layers": 1,\n  "partition_activations": true,\n  "synchronize_each_layer": true,\n\n  "gradient_clipping": 1.0,\n  "weight_decay": 0.1,\n  "hidden_dropout": 0,\n  "attention_dropout": 0,\n\n  "fp16": {\n    "fp16": true,\n    "enabled": true,\n    "loss_scale": 0,\n    "loss_scale_window": 1000,\n    "initial_scale_power": 12,\n    "hysteresis": 2,\n    "min_loss_scale": 1\n  },\n\n  "train-iters": 143000,\n  "lr-decay-iters": 143000,\n  "distributed-backend": "nccl",\n  "lr-decay-style": "cosine",\n  "warmup": 0.01,\n  "checkpoint-factor": 1000,\n  # "extra-save-iters": [0,1,2,4,8,16,32,64,128,256,512],\n  "eval-interval": 1, # changed from 40000\n  "eval-iters": 1, # changed from 10\n  "exit_interval": 35010,\n\n  "log_interval": 10,\n  "steps_per_print": 10,\n  "wall_clock_breakdown": true,\n\n  "tokenizer_type": "HFTokenizer"\n}\n'}updated
  data_impl ....................... mmap........................updated
  data_path ....................... /mnt/ssd-1/data/pile_20B_tokenizer/pile_20B_tokenizer_text_documentupdated
  dynamic_loss_scale .............. True........................updated
  eval_interval ................... 1...........................updated
  eval_iters ...................... 1...........................updated
  eval_tasks ...................... ['/mnt/hdd-0/alyssaloo/local_tasks.py']updated
  exit_interval ................... 35010.......................updated
  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated
  gas ............................. 1...........................updated
  global_num_gpus ................. 8...........................updated
  gpt_j_residual .................. True........................updated
  hidden_dropout .................. 0...........................updated
  hidden_size ..................... 768.........................updated
  init_method ..................... small_init..................updated
  is_pipe_parallel ................ True........................updated
  load ............................ /mnt/hdd-0/alyssaloo/pythia-v2-160Mupdated
  log_interval .................... 10..........................updated
  lr .............................. 0.0006......................updated
  lr_decay_iters .................. 143000......................updated
  lr_decay_style .................. cosine......................updated
  max_position_embeddings ......... 2048........................updated
  min_lr .......................... 6e-05.......................updated
  no_weight_tying ................. True........................updated
  num_attention_heads ............. 12..........................updated
  num_layers ...................... 12..........................updated
  num_workers ..................... 1...........................updated
  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-08}}updated
  optimizer_type .................. Adam........................updated
  output_layer_init_method ........ wang_init...................updated
  partition_activations ........... True........................updated
  pipe_parallel_size .............. 1...........................updated
  pos_emb ......................... rotary......................updated
  precision ....................... fp16........................updated
  rotary_pct ...................... 0.25........................updated
  save_iters ...................... [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 37000, 38000, 39000, 40000, 41000, 42000, 43000, 44000, 45000, 46000, 47000, 48000, 49000, 50000, 51000, 52000, 53000, 54000, 55000, 56000, 57000, 58000, 59000, 60000, 61000, 62000, 63000, 64000, 65000, 66000, 67000, 68000, 69000, 70000, 71000, 72000, 73000, 74000, 75000, 76000, 77000, 78000, 79000, 80000, 81000, 82000, 83000, 84000, 85000, 86000, 87000, 88000, 89000, 90000, 91000, 92000, 93000, 94000, 95000, 96000, 97000, 98000, 99000, 100000, 101000, 102000, 103000, 104000, 105000, 106000, 107000, 108000, 109000, 110000, 111000, 112000, 113000, 114000, 115000, 116000, 117000, 118000, 119000, 120000, 121000, 122000, 123000, 124000, 125000, 126000, 127000, 128000, 129000, 130000, 131000, 132000, 133000, 134000, 135000, 136000, 137000, 138000, 139000, 140000, 141000, 142000]updated
  scaled_upper_triang_masked_softmax_fusion  True...............updated
  seq_length ...................... 2048........................updated
  sparsity_config ................. {}..........................updated
  synchronize_each_layer .......... True........................updated
  text_gen_type ................... unconditional...............updated
  tokenizer_type .................. HFTokenizer.................updated
  train_batch_size ................ 256.........................updated
  train_iters ..................... 143000......................updated
  train_micro_batch_size_per_gpu .. 32..........................updated
  user_script ..................... /mnt/hdd-0/alyssaloo/gpt-neox/neox-evaluate.pyupdated
  vocab_file ...................... /mnt/ssd-1/data/20B_tokenizer.jsonupdated
  wall_clock_breakdown ............ True........................updated
  weight_decay .................... 0.1.........................updated
  zero_allgather_bucket_size ...... 500000000...................updated
  zero_contiguous_gradients ....... True........................updated
  zero_optimization ............... {'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
  zero_reduce_bucket_size ......... 500000000...................updated
  zero_reduce_scatter ............. True........................updated
  zero_stage ...................... 1...........................updated
  activation ...................... gelu........................default
  activation_checkpointing ........ None........................default
  adlr_autoresume ................. False.......................default
  adlr_autoresume_interval ........ 1000........................default
  amp ............................. None........................default
  apply_query_key_layer_scaling ... False.......................default
  attention_softmax_in_fp32 ....... False.......................default
  autotuning ...................... None........................default
  autotuning_run .................. None........................default
  base_shapes_file ................ None........................default
  bf16 ............................ None........................default
  bias_dropout_fusion ............. False.......................default
  char_level_ppl .................. False.......................default
  checkpoint ...................... None........................default
  checkpoint_in_cpu ............... False.......................default
  checkpoint_num_layers ........... 1...........................default
  checkpoint_scale ................ linear......................default
  checkpoint_validation_with_forward_pass  False................default
  comment ......................... None........................default
  comms_logger .................... None........................default
  communication_data_type ......... None........................default
  compression_training ............ None........................default
  contiguous_checkpointing ........ False.......................default
  coord_check ..................... False.......................default
  csv_monitor ..................... None........................default
  curriculum_learning ............. None........................default
  curriculum_seqlen ............... 0...........................default
  data_efficiency ................. None........................default
  data_types ...................... None........................default
  deepscale ....................... False.......................default
  deepscale_config ................ None........................default
  deepspeed ....................... True........................default
  deepspeed_activation_checkpointing  True......................default
  deepspeed_extra_args ............ None........................default
  deepspeed_mpi ................... False.......................default
  deepspeed_slurm ................. False.......................default
  detect_nvlink_pairs ............. False.......................default
  distributed_backend ............. nccl........................default
  do_test ......................... None........................default
  do_train ........................ None........................default
  do_valid ........................ None........................default
  dump_state ...................... False.......................default
  elasticity ...................... None........................default
  eod_mask_loss ................... False.......................default
  eval_results_prefix ............. ............................default
  exclude ......................... None........................default
  extra_save_iters ................ None........................default
  finetune ........................ False.......................default
  flops_profiler .................. None........................default
  force_multi ..................... False.......................default
  fp16_lm_cross_entropy ........... False.......................default
  fp32_allreduce .................. False.......................default
  git_hash ........................ edfa39b.....................default
  gmlp_attn_dim ................... 64..........................default
  gpt_j_tied ...................... False.......................default
  gradient_accumulation_steps ..... 1...........................default
  gradient_clipping ............... 1.0.........................default
  gradient_noise_scale_cpu_offload  False.......................default
  gradient_noise_scale_n_batches .. 5...........................default
  gradient_predivide_factor ....... 1.0.........................default
  hostfile ........................ None........................default
  hysteresis ...................... 2...........................default
  include ......................... None........................default
  init_method_std ................. 0.02........................default
  iteration ....................... None........................default
  keep_last_n_checkpoints ......... None........................default
  label_data_paths ................ None........................default
  launcher ........................ pdsh........................default
  layernorm_epsilon ............... 1e-05.......................default
  lazy_mpu_init ................... False.......................default
  local_rank ...................... None........................default
  log_dir ......................... None........................default
  log_grad_norm ................... False.......................default
  log_grad_pct_zeros .............. False.......................default
  log_gradient_noise_scale ........ False.......................default
  log_optimizer_states ............ False.......................default
  log_param_norm .................. False.......................default
  loss_scale ...................... None........................default
  loss_scale_window ............... 1000.0......................default
  make_vocab_size_divisible_by .... 128.........................default
  master_addr ..................... None........................default
  master_port ..................... 29500.......................default
  maximum_tokens .................. 64..........................default
  merge_file ...................... None........................default
  min_scale ....................... 1.0.........................default
  mlp_type ........................ regular.....................default
  mmap_warmup ..................... False.......................default
  model_parallel_size ............. 1...........................default
  mup_attn_temp ................... 1.0.........................default
  mup_embedding_mult .............. 1.0.........................default
  mup_init_scale .................. 1.0.........................default
  mup_output_temp ................. 1.0.........................default
  mup_rp_embedding_mult ........... 1.0.........................default
  mup_width_scale ................. 2...........................default
  no_load_optim ................... False.......................default
  no_load_rng ..................... False.......................default
  no_save_optim ................... False.......................default
  no_save_rng ..................... False.......................default
  no_ssh_check .................... False.......................default
  norm ............................ layernorm...................default
  num_gpus ........................ None........................default
  num_nodes ....................... -1..........................default
  num_samples ..................... 1...........................default
  num_unique_layers ............... None........................default
  onnx_safe ....................... False.......................default
  opt_pos_emb_offset .............. 0...........................default
  output_layer_parallelism ........ column......................default
  override_lr_scheduler ........... False.......................default
  padded_vocab_size ............... None........................default
  param_sharing_style ............. grouped.....................default
  pipe_partition_method ........... type:transformer|mlp........default
  prescale_gradients .............. False.......................default
  profile_backward ................ False.......................default
  prompt_end ...................... 
...........................default
  rank ............................ None........................default
  recompute ....................... False.......................default
  return_logits ................... False.......................default
  rms_norm_epsilon ................ 1e-08.......................default
  rotary_emb_base ................. 10000.......................default
  rpe_max_distance ................ 128.........................default
  rpe_num_buckets ................. 32..........................default
  sample_input_file ............... None........................default
  sample_output_file .............. samples.txt.................default
  save ............................ None........................default
  save_base_shapes ................ False.......................default
  scaled_masked_softmax_fusion .... False.......................default
  scalenorm_epsilon ............... 1e-08.......................default
  scheduler ....................... None........................default
  seed ............................ 1234........................default
  short_seq_prob .................. 0.1.........................default
  soft_prompt_tuning .............. None........................default
  sparse_attention ................ None........................default
  sparse_gradients ................ False.......................default
  split ........................... 969, 30, 1..................default
  steps_per_print ................. 10..........................default
  temperature ..................... 0.0.........................default
  tensorboard ..................... None........................default
  tensorboard_dir ................. None........................default
  test_data_paths ................. None........................default
  test_data_weights ............... None........................default
  top_k ........................... 0...........................default
  top_p ........................... 0.0.........................default
  train_data_paths ................ None........................default
  train_data_weights .............. None........................default
  use_bias_in_attn_linear ......... True........................default
  use_bias_in_norms ............... True........................default
  use_bnb_optimizer ............... False.......................default
  use_checkpoint_lr_scheduler ..... False.......................default
  use_cpu_initialization .......... False.......................default
  use_mup ......................... False.......................default
  use_shared_fs ................... True........................default
  use_wandb ....................... None........................default
  valid_data_paths ................ None........................default
  valid_data_weights .............. None........................default
  wandb ........................... None........................default
  wandb_group ..................... None........................default
  wandb_host ...................... https://api.wandb.ai........default
  wandb_init_all_ranks ............ False.......................default
  wandb_project ................... neox........................default
  wandb_team ...................... None........................default
  warmup .......................... 0.01........................default
  weight_by_num_documents ......... False.......................default
  weighted_sampler_alpha .......... 0.3.........................default
  world_size ...................... None........................default
---------------- end of arguments ----------------
NeoXArgs.configure_distributed_args() using world size: 1 and model-parallel size: 1 
[2023-07-17 23:58:43,371] [WARNING] [runner.py:199:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-07-17 23:58:43,371] [INFO] [runner.py:553:main] cmd = /mnt/hdd-0/alyssaloo/neox-env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /mnt/hdd-0/alyssaloo/gpt-neox/neox-evaluate.py --deepspeed_config eyJ0cmFpbl9iYXRjaF9zaXplIjogMjU2LCAidHJhaW5fbWljcm9fYmF0Y2hfc2l6ZV9wZXJfZ3B1IjogMzIsICJvcHRpbWl6ZXIiOiB7InR5cGUiOiAiQWRhbSIsICJwYXJhbXMiOiB7ImxyIjogMC4wMDA2LCAiYmV0YXMiOiBbMC45LCAwLjk1XSwgImVwcyI6IDFlLTA4fX0sICJmcDE2IjogeyJmcDE2IjogdHJ1ZSwgImVuYWJsZWQiOiB0cnVlLCAibG9zc19zY2FsZSI6IDAsICJsb3NzX3NjYWxlX3dpbmRvdyI6IDEwMDAsICJpbml0aWFsX3NjYWxlX3Bvd2VyIjogMTIsICJoeXN0ZXJlc2lzIjogMiwgIm1pbl9sb3NzX3NjYWxlIjogMX0sICJ6ZXJvX29wdGltaXphdGlvbiI6IHsic3RhZ2UiOiAxLCAiYWxsZ2F0aGVyX3BhcnRpdGlvbnMiOiB0cnVlLCAiYWxsZ2F0aGVyX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAib3ZlcmxhcF9jb21tIjogdHJ1ZSwgInJlZHVjZV9zY2F0dGVyIjogdHJ1ZSwgInJlZHVjZV9idWNrZXRfc2l6ZSI6IDUwMDAwMDAwMCwgImNvbnRpZ3VvdXNfZ3JhZGllbnRzIjogdHJ1ZSwgImNwdV9vZmZsb2FkIjogZmFsc2V9LCAid2FsbF9jbG9ja19icmVha2Rvd24iOiB0cnVlfQ== --megatron_config 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
Setting ds_accelerator to cuda (auto detect)
[2023-07-17 23:58:44,311] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2023-07-17 23:58:44,311] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2023-07-17 23:58:44,311] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2023-07-17 23:58:44,311] [INFO] [launch.py:163:main] dist_world_size=8
[2023-07-17 23:58:44,311] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Setting ds_accelerator to cuda (auto detect)
Setting ds_accelerator to cuda (auto detect)
Setting ds_accelerator to cuda (auto detect)
Setting ds_accelerator to cuda (auto detect)
Setting ds_accelerator to cuda (auto detect)
Setting ds_accelerator to cuda (auto detect)
Setting ds_accelerator to cuda (auto detect)
Setting ds_accelerator to cuda (auto detect)
[2023-07-17 23:58:50,386] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-17 23:58:50,386] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-07-17 23:58:50,388] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-17 23:58:50,388] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-07-17 23:58:50,772] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-17 23:58:50,772] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-07-17 23:58:50,840] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-17 23:58:50,840] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-07-17 23:58:51,024] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-17 23:58:51,025] [INFO] [comm.py:594:init_distributed] cdb=None
NeoXArgs.configure_distributed_args() using world size: 8 and model-parallel size: 1 
> building HFTokenizer tokenizer ...
[2023-07-17 23:58:51,201] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-17 23:58:51,201] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-07-17 23:58:51,220] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-17 23:58:51,221] [INFO] [comm.py:594:init_distributed] cdb=None
 > padded vocab (size: 50277) with 27 dummy tokens (new size: 50304)
> initializing torch distributed ...
[2023-07-17 23:58:51,257] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-17 23:58:51,257] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-07-17 23:58:51,257] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-07-17 23:58:53,375] [INFO] [launch.py:314:sigkill_handler] Killing subprocess 26341
[2023-07-17 23:58:53,378] [INFO] [launch.py:314:sigkill_handler] Killing subprocess 26342
[2023-07-17 23:58:53,473] [INFO] [launch.py:314:sigkill_handler] Killing subprocess 26343
[2023-07-17 23:58:53,528] [INFO] [launch.py:314:sigkill_handler] Killing subprocess 26344
[2023-07-17 23:58:53,583] [INFO] [launch.py:314:sigkill_handler] Killing subprocess 26345
[2023-07-17 23:58:53,679] [INFO] [launch.py:314:sigkill_handler] Killing subprocess 26346
[2023-07-17 23:58:53,777] [INFO] [launch.py:314:sigkill_handler] Killing subprocess 26348
[2023-07-17 23:58:53,872] [INFO] [launch.py:314:sigkill_handler] Killing subprocess 26351
[2023-07-17 23:58:53,928] [ERROR] [launch.py:320:sigkill_handler] ['/mnt/hdd-0/alyssaloo/neox-env/bin/python', '-u', '/mnt/hdd-0/alyssaloo/gpt-neox/neox-evaluate.py', '--local_rank=7', '--deepspeed_config', 'eyJ0cmFpbl9iYXRjaF9zaXplIjogMjU2LCAidHJhaW5fbWljcm9fYmF0Y2hfc2l6ZV9wZXJfZ3B1IjogMzIsICJvcHRpbWl6ZXIiOiB7InR5cGUiOiAiQWRhbSIsICJwYXJhbXMiOiB7ImxyIjogMC4wMDA2LCAiYmV0YXMiOiBbMC45LCAwLjk1XSwgImVwcyI6IDFlLTA4fX0sICJmcDE2IjogeyJmcDE2IjogdHJ1ZSwgImVuYWJsZWQiOiB0cnVlLCAibG9zc19zY2FsZSI6IDAsICJsb3NzX3NjYWxlX3dpbmRvdyI6IDEwMDAsICJpbml0aWFsX3NjYWxlX3Bvd2VyIjogMTIsICJoeXN0ZXJlc2lzIjogMiwgIm1pbl9sb3NzX3NjYWxlIjogMX0sICJ6ZXJvX29wdGltaXphdGlvbiI6IHsic3RhZ2UiOiAxLCAiYWxsZ2F0aGVyX3BhcnRpdGlvbnMiOiB0cnVlLCAiYWxsZ2F0aGVyX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAib3ZlcmxhcF9jb21tIjogdHJ1ZSwgInJlZHVjZV9zY2F0dGVyIjogdHJ1ZSwgInJlZHVjZV9idWNrZXRfc2l6ZSI6IDUwMDAwMDAwMCwgImNvbnRpZ3VvdXNfZ3JhZGllbnRzIjogdHJ1ZSwgImNwdV9vZmZsb2FkIjogZmFsc2V9LCAid2FsbF9jbG9ja19icmVha2Rvd24iOiB0cnVlfQ==', '--megatron_config', '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'] exits with return code = 1
